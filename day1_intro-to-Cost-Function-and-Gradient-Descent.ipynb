{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the following notations:\n",
    "\n",
    "- $m$ = no. of training examples\n",
    "- $n$ = no. of features corresponding to each training example\n",
    "- $x^i$ = input(feature) of the $i^{th}$ training example\n",
    "- $x^i_j$ = value of the feature $j$ in the $i^{th}$ training example\n",
    "\n",
    "We will stick to this notation (and others as we discuss further in this notebook) throughout all our future discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for understanding cost function and its role, consider a linear regression model with an input vector $x^i = (X_1 , X_2)$, and a real-valued output $y$. The linear regression model has the form:\n",
    "\n",
    "$$y_\\text{pred} = f_\\theta(X) = \\theta_0 + \\theta_1X_1 + \\theta_2X_2$$\n",
    "\n",
    "Here:\n",
    "- $f_\\theta(X)$ is called the **hypothesis function**\n",
    "- $[\\theta_1, \\theta_2]$ are called **model weights/parameters** (generally) OR **coefficients** (in linear regression)\n",
    "- $\\theta_0$ is called **bias** (generally) OR **intercept** (in linear regression)\n",
    "\n",
    "We consider a set of training data $(x_1 , y_1 ) \\dots (x_m , y_m )$ from which to estimate the parameters $\\theta$. Here m is the no of training examples we have. \n",
    "\n",
    "The Linear model can be plotted as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/lr.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X (independent variables) and y (dependent variable).\n",
    "\n",
    "The function that defines the difference between your actual value ($y_t$) and the predicted value ($y_p$). \n",
    "\n",
    "$$\n",
    "y_t = y_p + e\n",
    "$$\n",
    "\n",
    "In case of Linear regression with $y_p = f_\\theta(X) = \\theta_0 + \\theta_1X_1 + \\theta_2X_2$\n",
    "\n",
    "The error can be represented as:\n",
    "\n",
    "$$\n",
    "error = y_t - y_p = y_t - (\\theta_0 + \\theta_1X_1 + \\theta_2X_2)\n",
    "$$\n",
    "\n",
    "A more ML-ly term for error is **residual** and so going forward we will use that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically the errors can be represented as: \n",
    "\n",
    "<center><img src=\"images/error.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have different kinds of error like- total error, mean error, mean squared error etc.\n",
    "\n",
    "**Total error:** Summation of the absolute difference between predicted and actual value for all the data points. Mathematically, this is\n",
    "\n",
    "<img src=\"images/total_err.png\">\n",
    "\n",
    "**Mean error:** Total error divided by number of data points. Mathematically, this is\n",
    "\n",
    "<img src=\"images/mean_err.png\">\n",
    "\n",
    "**Mean squared error:** Summation of the square of absolute difference divided by number of data points. Mathematically, this is\n",
    "\n",
    "<img src=\"images/mean_sq_err.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cost function is a function of the errors/residuals , and in turn the function of the parameters. The same can be represented as:\n",
    "\n",
    "cost function = $ J_\\theta = f(\\theta_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use consider the MSE as our cost/loss funtion here. The same can be represented as:\n",
    "\n",
    "#### Mean Square Errors:\n",
    "$$J_\\theta =  \\frac{1}{m}\\sum_{i=1}^me_i^2 = \\frac{1}{m}\\sum_{i=1}^m(y_t - \\theta_0 - \\theta_1X_1 - \\theta_2X_2)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of any model training in machine learning is to **minimise the associated cost function**: $ minimise \\ J_\\theta $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before jumping into \"Gradient Descent\", first lets understand what is \"Gradient\" : \n",
    "\n",
    " - A gradient is an extension of partial derivatives. Gradients take the partial derivatives of each variable in a function and then places each partial derivative in a vector. The gradient value is zero at a local maximum or local minimum (because there is no single direction of increase) -  also referred to as **convergence**.\n",
    " \n",
    " \n",
    " - In mathematics, the **gradient is a multi-variable generalization of the derivative**. While a derivative can be defined on functions of a single variable, for functions of several variables, the gradient takes its place. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued.\n",
    " \n",
    " \n",
    " - **Gradient** of a function (at any given point) gives the direction of the **steepest ascent**, i.e. the direction to move if you want to increase the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent is an **optimisation algorithm** that attempts to find the **local or global minima** of a **convex** function by using its **partial derivatives**. In ML it is used to __optimise the cost function__ (reduce our total error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before proceeding, let us first understand what is a convex function:\n",
    "\n",
    "A convex function in its simplest form have the useful property that local minima is the global minima.\n",
    "<center><img src=\"images/convexity.png\" width=\"500\"/></center>\n",
    "\n",
    "Geometrically, a function is convex if a line segment drawn from any point (x, f(x)) to another point (y, f(y)) -- called the chord from x to y -- lies on or above the graph of f, as in the picture below:\n",
    "\n",
    "<center><img src=\"images/convex_def.png\" width=\"500\"/></center>\n",
    "\n",
    "A function is **concave** if -f is convex -- i.e. if the chord from x to y lies on or below the graph of f.\n",
    "<center><img src=\"images/concave.png\" width=\"500\"/></center>\n",
    "\n",
    "A **non-convex** function \"curves up and down\" -- it is neither convex nor concave.  A familiar example is the sine function:\n",
    "<center><img src=\"images/non_convex_def.png\" width=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind \"Gradient Descent\" is very simple — to reach the global optimum of a convex function from any point, we need to move in the direction **opposite** to that of greatest increase of the function. As the function is convex, this strategy will always take us to the global optimum. \n",
    "\n",
    "<center><img src=\"images/gd1.png\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now understand this in terms of the cost function $J_\\theta$\n",
    "\n",
    "Now, we know that the _direction_ of **greatest increase of any function** is determined by taking the partial derivatives of the function with respect to every variable (aka the gradient). For example, let us say we wish to optimise a convex function $J(\\theta)$, where $\\theta = (\\theta_1, \\theta_2, \\ldots, \\theta_n)^T$. Let us say, we start at a point $s \\in \\mathbb{R}^d$. Then, the direction of greatest increase of $J$ at $s$ is given by \n",
    "\n",
    "$$\n",
    "\\nabla J(s) = \\left(\\frac{\\partial J(s)}{\\partial \\theta_1}, \\frac{\\partial J(s)}{\\partial \\theta_2}, \\ldots ,\\frac{\\partial J(s)}{\\partial \\theta_n} \\right)^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to **decrease the function**, we take the **negative gradient**. The length of the gradient function is an indication of how step the slope is. \n",
    " \n",
    "With this derivative, we design an update rule, which asks us to move in the direction opposite to the direction of greatest increase. By repeatedly applying this rule, we hope to reach the global minimum. We hence move to $t$, which is given by\n",
    "\n",
    "$$\n",
    "t = s - \\eta \\nabla J(s),\n",
    "$$\n",
    "where $\\eta$ is a parameter called the *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T16:45:33.568438Z",
     "start_time": "2019-11-08T16:45:33.185828Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "#https://www.youtube.com/watch?v=vWFjqgb-ylQ\n",
    "YouTubeVideo(\"vWFjqgb-ylQ\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient with level curves\n",
    "![Gradient descent on a 2D convex function](images/gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Liner regression problem, to find the equation of the straight line $f_\\theta(x) = \\theta_0 + \\theta_1X_1 + \\theta_2X_2$ that best fits our data points. \n",
    "\n",
    "First lets create dummy data around this model:\n",
    "- m = 50\n",
    "- n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:21:41.563227Z",
     "start_time": "2019-11-09T11:21:41.066027Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:21:41.604775Z",
     "start_time": "2019-11-09T11:21:41.598734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50,), (50,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_1=np.random.randint(low=1,high=20,size=(50,))\n",
    "feature_2=np.random.randint(low=1,high=20,size=(50,)) \n",
    "\n",
    "feature_1.shape, feature_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:26:44.510105Z",
     "start_time": "2019-11-09T11:26:44.505693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:23:53.815045Z",
     "start_time": "2019-11-09T11:23:53.808313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 3), array([[ 1,  3, 19],\n",
       "        [ 1, 15,  3],\n",
       "        [ 1, 16,  1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array(feature_1).reshape(50,1)\n",
    "x0 = np.ones_like(feature_1).reshape(50,1)\n",
    "x2 = np.array(feature_2).reshape(50,1)\n",
    "\n",
    "x = np.hstack((x0,x1,x2))\n",
    "x.shape, x[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:35:26.730154Z",
     "start_time": "2019-11-09T11:35:26.717294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-66.08171462,  21.78842588,  31.79700356])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true=3+2*x[:,1]-4*x[:,2]+np.random.random((50,))\n",
    "print(y_true.shape)\n",
    "y_true[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:36:18.594897Z",
     "start_time": "2019-11-09T11:36:18.392331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[:,1], y_true, marker='o', c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:36:42.114336Z",
     "start_time": "2019-11-09T11:36:41.939229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[:,2], y_true, marker='o', c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T11:46:24.690528Z",
     "start_time": "2019-11-09T11:46:24.686301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(x)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that we are trying to minimize in this case is:\n",
    "\n",
    "$J(\\theta_0,\\theta_1, \\theta_2) = {1 \\over m} \\sum\\limits_{i=1}^m (f_\\theta(x^i)-y^i)^2$\n",
    "\n",
    "In this case, our gradient will be defined in three dimensions as:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1, \\theta_2) = \\frac{1}{2m}  \\sum\\limits_{i=1}^m (f_\\theta(x^i)-y^i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1, \\theta_2) = \\frac{1}{2m}  \\sum\\limits_{i=1}^m ((f_\\theta(x^i)-y^i) \\cdot x^i_1)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_2} J(\\theta_0,\\theta_1, \\theta_2) = \\frac{1}{2m}  \\sum\\limits_{i=1}^m ((f_\\theta(x^i)-y^i) \\cdot x^i_2)$\n",
    "\n",
    "\n",
    "The weight matrix will be updated following the equation:\n",
    "\n",
    "$w_{t+1} = w_t - \\eta \\nabla J(s)$\n",
    "\n",
    "Below, we set up our function for f, J and the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:08:15.602420Z",
     "start_time": "2019-11-09T12:08:15.598915Z"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda theta_0,theta_1, theta_2, x: theta_0 + theta_1*x[1] + theta_2*x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:14:34.267942Z",
     "start_time": "2019-11-09T12:14:34.263407Z"
    }
   },
   "outputs": [],
   "source": [
    "def J(x,y,m,theta_0,theta_1, theta_2):\n",
    "    \n",
    "    returnValue = 0\n",
    "    \n",
    "    for i in range(m):        \n",
    "        returnValue += (f(theta_0,theta_1, theta_2, x[i])-y[i])**2\n",
    "        \n",
    "    returnValue = returnValue/2m\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:22:40.440147Z",
     "start_time": "2019-11-09T12:22:40.434200Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_J(x,y,m,theta_0,theta_1, theta_2):\n",
    "    \n",
    "    returnValue = np.array([0.,0.,0.]) #initalise all the parameter values\n",
    "    \n",
    "    for i in range(m):\n",
    "        returnValue[0] += (f(theta_0,theta_1, theta_2, x[i])-y[i])\n",
    "        returnValue[1] += (f(theta_0,theta_1, theta_2, x[i])-y[i])*x[:,1][i]\n",
    "        returnValue[2] += (f(theta_0,theta_1, theta_2, x[i])-y[i])*x[:,2][i]\n",
    "        \n",
    "    returnValue = returnValue/(m)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run our gradient descent algorithm (without adaptive step sizes in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:23:28.714383Z",
     "start_time": "2019-11-09T12:23:28.710244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:23:43.520628Z",
     "start_time": "2019-11-09T12:23:43.516187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.zeros(x.shape[1]) #weight matrix\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:30:56.024976Z",
     "start_time": "2019-11-09T12:30:47.400870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:0, cost:957.549, neg_gradient:[  -5.8564124   -12.53980036 -131.23732716] weights:0.0 0.0 0.0\n",
      "steps:500, cost:1.355, neg_gradient:[ 0.1863209  -0.00788278 -0.00962592] weights:0.049 2.181 -3.853\n",
      "steps:1000, cost:1.287, neg_gradient:[ 0.18134406 -0.0093837  -0.00746149] weights:0.141 2.176 -3.857\n",
      "steps:1500, cost:1.223, neg_gradient:[ 0.17650993 -0.00913361 -0.00726253] weights:0.231 2.172 -3.861\n",
      "steps:2000, cost:1.162, neg_gradient:[ 0.17180466 -0.00889013 -0.00706893] weights:0.318 2.167 -3.865\n",
      "steps:2500, cost:1.104, neg_gradient:[ 0.16722483 -0.00865314 -0.0068805 ] weights:0.402 2.163 -3.868\n",
      "steps:3000, cost:1.049, neg_gradient:[ 0.16276708 -0.00842247 -0.00669708] weights:0.485 2.159 -3.871\n",
      "steps:3500, cost:0.998, neg_gradient:[ 0.15842816 -0.00819795 -0.00651855] weights:0.565 2.155 -3.875\n",
      "steps:4000, cost:0.948, neg_gradient:[ 0.1542049  -0.00797942 -0.00634479] weights:0.643 2.15 -3.878\n",
      "steps:4500, cost:0.902, neg_gradient:[ 0.15009422 -0.00776671 -0.00617565] weights:0.719 2.147 -3.881\n",
      "steps:5000, cost:0.858, neg_gradient:[ 0.14609313 -0.00755967 -0.00601103] weights:0.793 2.143 -3.884\n",
      "steps:5500, cost:0.816, neg_gradient:[ 0.14219869 -0.00735815 -0.00585079] weights:0.865 2.139 -3.887\n",
      "steps:6000, cost:0.777, neg_gradient:[ 0.13840807 -0.007162   -0.00569482] weights:0.936 2.135 -3.89\n",
      "steps:6500, cost:0.739, neg_gradient:[ 0.13471849 -0.00697108 -0.00554302] weights:1.004 2.132 -3.893\n",
      "steps:7000, cost:0.704, neg_gradient:[ 0.13112727 -0.00678525 -0.00539525] weights:1.07 2.128 -3.895\n",
      "steps:7500, cost:0.67, neg_gradient:[ 0.12763178 -0.00660438 -0.00525143] weights:1.135 2.125 -3.898\n",
      "steps:8000, cost:0.638, neg_gradient:[ 0.12422947 -0.00642832 -0.00511144] weights:1.198 2.122 -3.901\n",
      "steps:8500, cost:0.608, neg_gradient:[ 0.12091786 -0.00625696 -0.00497519] weights:1.259 2.119 -3.903\n",
      "steps:9000, cost:0.579, neg_gradient:[ 0.11769452 -0.00609017 -0.00484256] weights:1.319 2.116 -3.906\n",
      "steps:9500, cost:0.552, neg_gradient:[ 0.11455711 -0.00592782 -0.00471347] weights:1.377 2.113 -3.908\n",
      "steps:10000, cost:0.527, neg_gradient:[ 0.11150334 -0.0057698  -0.00458782] weights:1.434 2.11 -3.91\n",
      "steps:10500, cost:0.502, neg_gradient:[ 0.10853097 -0.005616   -0.00446553] weights:1.489 2.107 -3.913\n",
      "steps:11000, cost:0.479, neg_gradient:[ 0.10563783 -0.00546629 -0.00434649] weights:1.542 2.104 -3.915\n",
      "steps:11500, cost:0.458, neg_gradient:[ 0.10282182 -0.00532057 -0.00423062] weights:1.594 2.101 -3.917\n",
      "steps:12000, cost:0.437, neg_gradient:[ 0.10008087 -0.00517874 -0.00411785] weights:1.645 2.099 -3.919\n",
      "steps:12500, cost:0.417, neg_gradient:[ 0.09741299 -0.00504069 -0.00400807] weights:1.694 2.096 -3.921\n",
      "steps:13000, cost:0.399, neg_gradient:[ 0.09481623 -0.00490632 -0.00390123] weights:1.742 2.094 -3.923\n",
      "steps:13500, cost:0.381, neg_gradient:[ 0.0922887  -0.00477553 -0.00379723] weights:1.789 2.091 -3.925\n",
      "steps:14000, cost:0.364, neg_gradient:[ 0.08982853 -0.00464823 -0.00369601] weights:1.835 2.089 -3.927\n",
      "steps:14500, cost:0.349, neg_gradient:[ 0.08743396 -0.00452432 -0.00359749] weights:1.879 2.087 -3.929\n",
      "steps:15000, cost:0.334, neg_gradient:[ 0.08510321 -0.00440371 -0.00350159] weights:1.922 2.084 -3.931\n",
      "steps:15500, cost:0.32, neg_gradient:[ 0.08283459 -0.00428632 -0.00340824] weights:1.964 2.082 -3.932\n",
      "steps:16000, cost:0.306, neg_gradient:[ 0.08062645 -0.00417206 -0.00331739] weights:2.005 2.08 -3.934\n",
      "steps:16500, cost:0.293, neg_gradient:[ 0.07847717 -0.00406085 -0.00322896] weights:2.045 2.078 -3.936\n",
      "steps:17000, cost:0.281, neg_gradient:[ 0.07638519 -0.00395259 -0.00314288] weights:2.083 2.076 -3.937\n",
      "steps:17500, cost:0.27, neg_gradient:[ 0.07434897 -0.00384723 -0.0030591 ] weights:2.121 2.074 -3.939\n",
      "steps:18000, cost:0.259, neg_gradient:[ 0.07236703 -0.00374467 -0.00297755] weights:2.158 2.072 -3.94\n",
      "steps:18500, cost:0.249, neg_gradient:[ 0.07043793 -0.00364485 -0.00289818] weights:2.193 2.07 -3.942\n",
      "steps:19000, cost:0.239, neg_gradient:[ 0.06856025 -0.00354769 -0.00282092] weights:2.228 2.068 -3.943\n",
      "steps:19500, cost:0.23, neg_gradient:[ 0.06673262 -0.00345312 -0.00274573] weights:2.262 2.067 -3.945\n",
      "steps:20000, cost:0.221, neg_gradient:[ 0.06495372 -0.00336107 -0.00267253] weights:2.295 2.065 -3.946\n",
      "steps:20500, cost:0.213, neg_gradient:[ 0.06322223 -0.00327147 -0.00260129] weights:2.327 2.063 -3.947\n",
      "steps:21000, cost:0.205, neg_gradient:[ 0.0615369  -0.00318426 -0.00253195] weights:2.358 2.062 -3.948\n",
      "steps:21500, cost:0.198, neg_gradient:[ 0.0598965  -0.00309938 -0.00246445] weights:2.389 2.06 -3.95\n",
      "steps:22000, cost:0.191, neg_gradient:[ 0.05829982 -0.00301676 -0.00239876] weights:2.418 2.059 -3.951\n",
      "steps:22500, cost:0.184, neg_gradient:[ 0.05674571 -0.00293634 -0.00233481] weights:2.447 2.057 -3.952\n",
      "steps:23000, cost:0.178, neg_gradient:[ 0.05523303 -0.00285806 -0.00227257] weights:2.475 2.056 -3.953\n",
      "steps:23500, cost:0.172, neg_gradient:[ 0.05376067 -0.00278188 -0.00221199] weights:2.502 2.054 -3.954\n",
      "steps:24000, cost:0.166, neg_gradient:[ 0.05232756 -0.00270772 -0.00215303] weights:2.529 2.053 -3.955\n",
      "steps:24500, cost:0.161, neg_gradient:[ 0.05093265 -0.00263554 -0.00209563] weights:2.554 2.052 -3.957\n",
      "steps:25000, cost:0.156, neg_gradient:[ 0.04957493 -0.00256528 -0.00203977] weights:2.58 2.05 -3.958\n",
      "steps:25500, cost:0.151, neg_gradient:[ 0.0482534  -0.0024969  -0.00198539] weights:2.604 2.049 -3.959\n",
      "steps:26000, cost:0.147, neg_gradient:[ 0.04696709 -0.00243034 -0.00193247] weights:2.628 2.048 -3.96\n",
      "steps:26500, cost:0.142, neg_gradient:[ 0.04571508 -0.00236555 -0.00188096] weights:2.651 2.047 -3.961\n",
      "steps:27000, cost:0.138, neg_gradient:[ 0.04449644 -0.00230249 -0.00183081] weights:2.674 2.045 -3.961\n",
      "steps:27500, cost:0.134, neg_gradient:[ 0.04331029 -0.00224112 -0.00178201] weights:2.696 2.044 -3.962\n",
      "steps:28000, cost:0.131, neg_gradient:[ 0.04215576 -0.00218137 -0.00173451] weights:2.717 2.043 -3.963\n",
      "steps:28500, cost:0.127, neg_gradient:[ 0.041032   -0.00212322 -0.00168827] weights:2.738 2.042 -3.964\n",
      "steps:29000, cost:0.124, neg_gradient:[ 0.0399382  -0.00206662 -0.00164326] weights:2.758 2.041 -3.965\n",
      "steps:29500, cost:0.121, neg_gradient:[ 0.03887356 -0.00201153 -0.00159946] weights:2.778 2.04 -3.966\n",
      "steps:30000, cost:0.118, neg_gradient:[ 0.0378373  -0.00195791 -0.00155682] weights:2.797 2.039 -3.967\n",
      "steps:30500, cost:0.115, neg_gradient:[ 0.03682866 -0.00190572 -0.00151532] weights:2.815 2.038 -3.967\n",
      "steps:31000, cost:0.112, neg_gradient:[ 0.03584691 -0.00185492 -0.00147493] weights:2.834 2.037 -3.968\n",
      "steps:31500, cost:0.11, neg_gradient:[ 0.03489133 -0.00180547 -0.00143561] weights:2.851 2.036 -3.969\n",
      "steps:32000, cost:0.107, neg_gradient:[ 0.03396123 -0.00175734 -0.00139734] weights:2.869 2.035 -3.969\n",
      "steps:32500, cost:0.105, neg_gradient:[ 0.03305591 -0.0017105  -0.00136009] weights:2.885 2.034 -3.97\n",
      "steps:33000, cost:0.103, neg_gradient:[ 0.03217474 -0.0016649  -0.00132384] weights:2.902 2.034 -3.971\n",
      "steps:33500, cost:0.101, neg_gradient:[ 0.03131705 -0.00162052 -0.00128855] weights:2.917 2.033 -3.971\n",
      "steps:34000, cost:0.099, neg_gradient:[ 0.03048222 -0.00157732 -0.0012542 ] weights:2.933 2.032 -3.972\n",
      "steps:34500, cost:0.097, neg_gradient:[ 0.02966965 -0.00153527 -0.00122076] weights:2.948 2.031 -3.973\n",
      "steps:35000, cost:0.096, neg_gradient:[ 0.02887874 -0.00149435 -0.00118822] weights:2.963 2.03 -3.973\n",
      "steps:35500, cost:0.094, neg_gradient:[ 0.02810891 -0.00145451 -0.00115655] weights:2.977 2.03 -3.974\n",
      "steps:36000, cost:0.092, neg_gradient:[ 0.02735961 -0.00141574 -0.00112572] weights:2.991 2.029 -3.975\n",
      "steps:36500, cost:0.091, neg_gradient:[ 0.02663027 -0.001378   -0.00109571] weights:3.004 2.028 -3.975\n",
      "steps:37000, cost:0.089, neg_gradient:[ 0.02592038 -0.00134126 -0.0010665 ] weights:3.017 2.028 -3.976\n",
      "steps:37500, cost:0.088, neg_gradient:[ 0.02522942 -0.00130551 -0.00103807] weights:3.03 2.027 -3.976\n",
      "steps:38000, cost:0.087, neg_gradient:[ 0.02455687 -0.00127071 -0.0010104 ] weights:3.043 2.026 -3.977\n",
      "steps:38500, cost:0.086, neg_gradient:[ 0.02390225 -0.00123684 -0.00098346] weights:3.055 2.026 -3.977\n",
      "steps:39000, cost:0.085, neg_gradient:[ 0.02326509 -0.00120387 -0.00095725] weights:3.066 2.025 -3.978\n",
      "steps:39500, cost:0.084, neg_gradient:[ 0.0226449  -0.00117177 -0.00093173] weights:3.078 2.025 -3.978\n",
      "\n",
      "Local minimum occurs where:\n",
      "theta_0 = 3.0891277234665586\n",
      "theta_1 = 2.023925212065539\n",
      "theta_2 = -3.978552267625835\n",
      "This took 40000 steps to converge\n"
     ]
    }
   ],
   "source": [
    "n_k = 0.001 # learning rate\n",
    "#s_k = float(\"inf\")\n",
    "s_k = np.array([float(\"inf\"),float(\"inf\"),float(\"inf\")])\n",
    "\n",
    "num_steps = 40000\n",
    "for i in range(num_steps):\n",
    "    s_k = -grad_J(x,y_true,m,w[0],w[1], w[2])\n",
    "\n",
    "    if i%500==0:\n",
    "        cost = J(x,y_true,m,w[0],w[1], w[2])\n",
    "        print(\"steps:{}, cost:{}, neg_gradient:{} weights:{} {} {}\".format(i, round(cost,3), s_k, round(w[0],3),\n",
    "                                                                           round(w[1],3), round(w[2],3)))\n",
    "        \n",
    "    w = w + n_k * s_k\n",
    "\n",
    "print(\"\\nLocal minimum occurs where:\")\n",
    "print(\"theta_0 =\", w[0])\n",
    "print(\"theta_1 =\", w[1])\n",
    "print(\"theta_2 =\", w[2])\n",
    "print(\"This took\",num_steps,\"steps to converge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's see the actual values for $\\theta_0$, $\\theta_1$ and $\\theta_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T12:33:58.784079Z",
     "start_time": "2019-11-09T12:33:58.779027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual values for theta are:\n",
      "theta_0 = 3\n",
      "theta_1 = 2\n",
      "theta_2 = -4\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual values for theta are:\")\n",
    "print(\"theta_0 =\", 3)\n",
    "print(\"theta_1 =\", 2)\n",
    "print(\"theta_2 =\", -4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that our values are relatively close to the actual values (even though our method was pretty slow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the method above we need to calculate the gradient over all the m training samples, in every step of our algorithm. With the dummy dataset, this is not a big deal since there are only 50 data points. But imagine that we had 10 million data points. If this were the case, it would certainly make the method above far less efficient.\n",
    "\n",
    "In machine learning, the algorithm above is often called <b>batch gradient descent</b> (each step of gradient descent uses all the training examples) to contrast it with <b>mini-batch gradient descent</b> (which we will not go into here) and <b>stochastic gradient descent</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, in batch gradient descent, we must look at every example in the entire training set on every step. This can be quite slow if the training set is sufficiently large. In <b>stochastic gradient descent</b>, we update our values after looking at <i>each</i> item in the training set, so that we can start making progress right away. Recall the linear regression example above. In that example, we calculated the gradient for each of the three theta values as follows:\n",
    "\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1, \\theta_2) = \\frac{1}{2m}  \\sum\\limits_{i=1}^m (f_\\theta(x^i)-y^i)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1, \\theta_2) = \\frac{1}{2m}  \\sum\\limits_{i=1}^m ((f_\\theta(x^i)-y^i) \\cdot x^i_1)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_2} J(\\theta_0,\\theta_1, \\theta_2) = \\frac{1}{2m}  \\sum\\limits_{i=1}^m ((f_\\theta(x^i)-y^i) \\cdot x^i_2)$\n",
    "\n",
    "Where $f_\\theta(x) = \\theta_0 + \\theta_1X_1 + \\theta_2X_2$\n",
    "\n",
    "Then we followed this algorithm (where $\\alpha$ was a non-adapting stepsize):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $w_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla J(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_{k+1} = w_k + \\alpha s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;  <b>end for</b>\n",
    "\n",
    "When the sample data had 50 data points as in the example above, calculating the gradient was not very costly. But for very large data sets, this would not be the case. So instead, we consider a stochastic gradient descent algorithm for simple linear regression such as the following, where m is the size of the data set:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Randomly shuffle the data set <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>for</b> i = 1 to m <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\begin{bmatrix}\n",
    " \\theta_{1} \\\\ \n",
    " \\theta_2 \\\\ \n",
    " \\theta_3 \\\\ \n",
    " \\end{bmatrix}=\\begin{bmatrix}\n",
    " \\theta_1 \\\\ \n",
    " \\theta_2 \\\\\n",
    " \\theta_3 \\\\ \n",
    " \\end{bmatrix}-\\alpha\\begin{bmatrix}\n",
    " 2(f_\\theta(x^i)-y^i) \\\\ \n",
    " 2x^i_j(f_\\theta(x^i)-y^i) \\\\ \n",
    " 2x^i_j(f_\\theta(x^i)-y^i) \\\\ \n",
    " \\end{bmatrix}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <b>end for</b> <br> \n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>\n",
    "\n",
    "The update rule for weights goes as:\n",
    "$w_{t+1} = w_{t} - \\eta (y_{i} - \\hat{y}) x_{i}$\n",
    "\n",
    "Typically, with stochastic gradient descent, you will run through the entire data set 1 to 10 times (see value for k in line 2 of the pseudocode above), depending on how fast the data is converging and how large the data set is.\n",
    "\n",
    "With batch gradient descent, we must go through the entire data set before we make any progress. With this algorithm though, we can make progress right away and continue to make progress as we go through the data set. Therefore, stochastic gradient descent is often preferred when dealing with large data sets.\n",
    "\n",
    "Unlike gradient descent, stochastic gradient descent will tend to oscillate <i>near</i> a minimum value rather than continuously getting closer. It may never actually converge to the minimum though. One way around this is to slowly decrease the step size $\\alpha$ as the algorithm runs. However, this is less common than using a fixed $\\alpha$.\n",
    "\n",
    "Let's look at above example again, but this time with stochastic gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T17:28:18.254159Z",
     "start_time": "2019-11-08T17:28:18.248185Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_1=np.random.randint(low=1,high=20,size=(50,))\n",
    "feature_2=np.random.randint(low=1,high=20,size=(50,)) \n",
    "\n",
    "x1 = np.array(feature_1).reshape(50,1)\n",
    "x0 = np.ones_like(feature_1).reshape(50,1)\n",
    "x2 = np.array(feature_2).reshape(50,1)\n",
    "\n",
    "x = np.hstack((x0,x1,x2))\n",
    "\n",
    "y =3+2*x[:,1]-4*x[:,2]+np.random.random((50,))\n",
    "\n",
    "m = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll setup our h function and our cost function, which we will use to check how the value is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T17:28:19.093928Z",
     "start_time": "2019-11-08T17:28:19.089713Z"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda theta_0,theta_1, theta_2, x: theta_0 + theta_1*x[1] + theta_2*x[2]\n",
    "cost = lambda theta_0,theta_1, theta_2, x_i, y_i: 0.5*(f(theta_0,theta_1,theta_2,x_i)-y_i)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run our stochastic gradient descent algorithm. To see it's progress, we'll take a cost measurement at every step. We will run through the entire list 1000 times here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T17:29:06.355872Z",
     "start_time": "2019-11-08T17:29:06.183781Z"
    }
   },
   "outputs": [],
   "source": [
    "w=np.zeros(x.shape[1]) #weight matrix\n",
    "n_k = 0.001 # step size\n",
    "\n",
    "s_k = np.array([float(\"inf\"),float(\"inf\"),float(\"inf\")])\n",
    "\n",
    "num_steps=400\n",
    "for j in range(num_steps):\n",
    "    for i in range(m):\n",
    "        s_k[0] = (f(w[0],w[1],w[2],x[i])-y[i])\n",
    "        s_k[1] = (f(w[0],w[1],w[2],x[i])-y[i])*x[:,1][i]\n",
    "        s_k[2] = (f(w[0],w[1],w[2],x[i])-y[i])*x[:,2][i]\n",
    "        \n",
    "        s_k = (-1)*s_k\n",
    "        \n",
    "        w = w + n_k * s_k  \n",
    "            \n",
    "print(\"\\nLocal minimum occurs where:\")\n",
    "print(\"theta_0 =\", w[0])\n",
    "print(\"theta_1 =\", w[1])\n",
    "print(\"theta_2 =\", w[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our values for $\\theta_0$, $\\theta_1$ and $\\theta_2$ are close to their true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent (MB-GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all n training samples (GD), we compute the gradient from 1<k<n training samples (a common mini-batch size is k=50).\n",
    "\n",
    "MB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other Optimisation Strategies (other than Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Brent's method for optimisation. \n",
    "\n",
    "Brent's method does not require the function to optimize to be convex or derivable everywhere, and works as a combination of the secant method and parabola fittings, as follows:\n",
    "1. Take a, b such that f'(a) < 0 and f'(b) > 0\n",
    "2. Repeat:\n",
    "  * Compute c = (a+b)/2\n",
    "  * Compute the point d where the parabola that goes through a, b and c is minimal\n",
    "  * If f'(d) < 10^{-sthg}: stop\n",
    "  * Otherwise if f'(d) < 0: replace a with d, otherwise: replace b with d.\n",
    "\n",
    "<img src=\"images/brent.png\" width=\"600\"/>\n",
    "\n",
    "Illustration's source: Press, W. H., et al. (1992) \"Numerical Recipes in Fortran, The Art of Scientific Computing\", Cambridge University Press, Chapter 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To illustrate the above, we begin by defining a function to optimise:\n",
    "$$J(x) = (x - 1)^4 + x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# specify objective/cost function\n",
    "J = lambda x : (x - 1) ** 4 + x ** 2\n",
    "\n",
    "res = minimize_scalar(J, method='brent')\n",
    "print('xmin: %.02f, fval: %.02f, iter: %d' % (res.x, res.fun, res.nit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot curve\n",
    "x = np.linspace(res.x - 0.5, res.x + 0.5, 100)\n",
    "y = [J(val) for val in x]\n",
    "plt.plot(x, y, color='blue', label='f1')\n",
    "\n",
    "# plot optima\n",
    "plt.scatter(res.x, res.fun, color='orange', marker='x', label='opt')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### First and second-order characterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A twice-differentiable convex function has a positive semi-definite Hessian $\\mathbf{x} \\mapsto \\nabla^2 f(\\mathbf{x})$ and is minimized where the gradient $\\mathbf{x} \\mapsto \\nabla f(\\mathbf{x})$ is equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Newton's Method\n",
    "\n",
    "Newton's method is a second order method, minimising a quadratic Taylor approximation to the objective function at each point. It thus combines the gradient and Hessian matrix,\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} -\\alpha\\mathbf{H}_k^{-1}\\mathbf{g}_k$$\n",
    "\n",
    "where $\\mathbf{g}_k = \\nabla f(\\mathbf{x_k})$ and $\\mathbf{H}_k = \\nabla^2 f(\\mathbf{x_k})$. This is a multi-dimensional generalisation of the Newton root-finding method (here we are finding the root of the gradient). Newton's method typically involves a line search to optimise the size of the descent step, $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Newton's method uses a line search to optimise each descent step by choosing a step size $\\alpha$ that optimises the descent direction, that is, $$\\min_\\alpha f(\\mathbf{x_k} + \\alpha \\mathbf{d}_k),$$ where $\\mathbf{d}_k = \\nabla f({\\mathbf{x}_k})$ is the descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Conjugate gradient method\n",
    "\n",
    "The [conjugate gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method) is an alternative to gradient descent. For a linear system $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, this algorithm finds a solution as a linear combination of a set of *mutually conjugate vectors*, $\\mathbf{p}_d$, such that,\n",
    "\n",
    "$$\\mathbf{x}^* = \\sum_{d=1}^D \\alpha_d\\mathbf{p}_d$$ for the $D$ dimensions of the problem. The vectors are built determined one by one in a process similar to the [Gram Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). Particularly for a sparse system, a good approximate solution can be determined without constructing the entire conjugate set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. https://mccormickml.com/2014/03/04/gradient-descent-derivation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bias is the error rate of your model on the training dataset.\n",
    "- Bias is how much your model __under-fits__ the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you compute bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Bias = E[y_p - y_t]$$\n",
    "\n",
    "Expected difference between predicted and observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is a learners' tendency to learn the wrong thing\n",
    "-----    \n",
    "\n",
    "Which the following has higher bias?\n",
    "\n",
    "1. $y = \\theta _0$\n",
    "1. $y = \\theta _0 + \\theta _1x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is bad\n",
    "------\n",
    "\n",
    "An algorithm that has a good ability to fit the training data has __low__ bias.\n",
    "\n",
    "We want to minimize bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms with high bias \n",
    "-------\n",
    "\n",
    "- Produce simple models\n",
    "- Fail to capture meaningful patterns in the data\n",
    "- Under-fit their training data (also don't over-fit either)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to decrease bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Make the model more complex!  \n",
    "\n",
    "You can add more parameters:\n",
    "$y = \\theta _0 + \\theta _1x $  \n",
    "$y = \\theta _0 + \\theta _1x  + \\theta _2x$  \n",
    "$y = \\theta _0 + \\theta _1x  + \\theta _2x + \\theta _3x$  \n",
    "…\n",
    "\n",
    "Or increase model complexity by picking a different algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Larger set of features\n",
    "- Better features\n",
    "\n",
    "Both will increase your model's ability to fit the training dataset, thus lowering bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variance is the amount by which the model result will change for a small change in the input data.\n",
    "- If for a small change in input data, the model results change a lot than the model is said to have high variance\n",
    "- Variance is an algorithm's flexibility to learn patterns in the observed data.\n",
    "- If Variance is high,  that means our model __over-fits__ the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Complexity will Increase Variance \n",
    "------\n",
    "\n",
    "The more complex the model is, the more data points it will \"capture\". \n",
    "\n",
    "However, complexity will make the model \"move\" more to \"capture\" the data points, and hence its variance will be larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance is how much worse you do on the test dataset compared to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should you do if you have high variance?\n",
    "------\n",
    "\n",
    "1. Feature Selection\n",
    "1. Regularization\n",
    "1. Dimensionality Reduction\n",
    "1. Bagging methods (e.g., Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Vs Variance\n",
    "\n",
    "<center><img src=\"images/bias_var2.png\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Machine Learning:\n",
    "\n",
    "1. Low bias (model the  patterns in the observed data) \n",
    "1. Low variance (not sensitive to specificities of the observed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bias_var.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-variance trade-off: A balancing act\n",
    "------\n",
    "\n",
    "<center><img src=\"images/abstract_better.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Underfitting\n",
    "- Underfitting refers to not capturing enough patterns in the data. The model performs poorly both in the training and the test set.\n",
    "\n",
    "## 4. Overfitting\n",
    "- Overfitting refers: a)capturing noise and b) capturing patterns which do not generalize well to unseen data. The model performs extremely well to the training set but poorly on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-validation is a model validation techniques for assessing how the results of a statistical model will generalize to an independent data set. \n",
    "- The goal of cross-validation is to define a data set to test the model in the training phase (i.e. validation data set) in order to limit problems like overfitting,underfitting and get an insight on how the model will generalize to an independent data set. \n",
    "- It is important the validation and the training set to be drawn from the same distribution otherwise it would make things worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 for Validation: Train/Test split or Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this strategy, we simply split the data into two sets: train and test set so that the sample between train and test set do not overlap, if they do we simply can’t trust our model.\n",
    "\n",
    "<center><img src=\"images/cv1.png\" width=\"75%\"/></center>\n",
    "\n",
    "BUT:\n",
    "- What if the split we make isn’t random? What if one subset of our data has only people from a certain state, employees with a certain income level but not other income levels, only women or only people at a certain age? . This will result in overfitting, even though we’re trying to avoid it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: K-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be viewed as repeated holdout and we simply average scores after K different holdouts. Every data point gets to be in a validation set exactly once, and gets to be in a training set k-1times. This significantly reduces underfitting as we are using most of the data for fitting, and also significantly reduces overfitting as most of the data is also being used in validation set.\n",
    "\n",
    "<center><img src=\"images/cv2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameters & Hyperparameters - The difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters are learned during training and learned for a specific model on specific data.\n",
    "\n",
    "Hyperparameters are properties of the algorithm.\n",
    "\n",
    "Hyperparameters are set before the start of a training.\n",
    "\n",
    "-----\n",
    "\n",
    "Model parameters are always learned that is why it is Machine Learning.\n",
    "\n",
    "Hyperparameters can be picked or learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation metrics for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy \n",
    "- Recall\n",
    "- Precision\n",
    "- F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/pr_re.png\" width=\"80%\"/></center>\n",
    "\n",
    "- True Positives (TP): correctly predicted a succesfull outcome /  one \n",
    "label \n",
    "- True Negatives (TN): correctly predicted a lack of an outcome / other label \n",
    "- False Positives (FP): incorrectly predicted a succesfull outcome (a \"Type I error\")\n",
    "- False Negatives (FN): incorrectly predicted lack of an outcome (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/pregnant.jpg\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "$$Accuracy = \\frac{All\\ Correct}{Total}$$\n",
    "\n",
    "- Fraction of observations classified correctly\n",
    "- 1 - error rate\n",
    "\n",
    "#### What is the biggest limitation of accuracy?\n",
    "\n",
    "Accuracy is an overall measure (ignores which classes were correctly predicted). It does not tell you what \"types\" of errors your classifier is making\n",
    "\n",
    "It is effected by class imbalances, when there is much one group than another group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "$$Precision = \\frac{Class\\ Correct}{Class\\ Total\\ Predicted}$$\n",
    "\n",
    "Fraction of labeled items assigned to a class that are actually members of that class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "$$Recall = \\frac{Class\\ Correct}{Class\\ Total\\ Actual}$$\n",
    "\n",
    "Fraction of labeled items in a class that are classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "------\n",
    "\n",
    "<center><img src=\"images/confu_matrix.png\" width=\"40%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/statistical-classification-metrics.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "<center><img src=\"images/p_r.png\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F<sub>1</sub> score\n",
    "\n",
    "The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. \n",
    "\n",
    "\n",
    "$$F_1\\ Score = 2•\\frac{Precision•Recall}{Precision+Recall}$$\n",
    "\n",
    "A single metric that combines precision and recall.\n",
    "\n",
    "In Machine Learning, we want a single metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized F score\n",
    "\n",
    "<center><img src=\"images/f_score_2.png\" width=\"75%\"/></center>\n",
    "\n",
    "F<sub>1</sub> weighs recall and precision equally.\n",
    "\n",
    "F<sub>0.5</sub> weighs recall lower than precision (by reducing the influence of false negatives).\n",
    "\n",
    "F<sub>2</sub> weighs recall higher than precision (by placing more emphasis on false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC (receiver operating characteristic) curve \n",
    "----\n",
    "\n",
    "ROC or Receiver Operating Characteristic curve is a graph of true positive rates vs the false positive rate at various thresholds. \n",
    "It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the probability that it will trigger a false alarm (false positives).\n",
    "\n",
    "\n",
    "<center><img src=\"images/roc_first.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

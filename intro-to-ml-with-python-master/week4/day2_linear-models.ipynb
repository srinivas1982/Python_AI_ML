{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T08:33:53.101182Z",
     "start_time": "2019-11-08T08:33:52.240686Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear regression model with an input vector $x^i = (X_1 , X_2 , \\dots , X_n )$, and a real-valued output $y$. The linear regression model has the form:\n",
    "\n",
    "$$\\large f_\\theta(X) = \\theta_0 + \\sum_{i=1}^mx^i\\theta^i$$\n",
    "\n",
    "Here:\n",
    "- $[\\theta_1, \\theta_2, ... , \\theta_n]$ are called **model weights** (generally) OR **coefficients** (in linear regression)\n",
    "- $\\theta_0$ is called **bias** (generally) OR **intercept** (in linear regression)\n",
    "\n",
    "We consider a set of training data $(x_1 , y_1 ) \\dots (x_m , y_m )$ from which to estimate the parameters $\\theta$. Here m is the no of training examples we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model relies on several assumptions:\n",
    "\n",
    " - The independent variable is not random.\n",
    " - The variance of the error term is constant across observations. This is important for evaluating the goodness of the fit.\n",
    " - The errors are not autocorrelated. The Durbin-Watson statistic detects this; if it is close to 2, there is no autocorrelation.\n",
    " - The errors are normally distributed. If this does not hold, we cannot use some of the statistics, such as the F-test.\n",
    "\n",
    "If we confirm that the necessary assumptions of the regression model are satisfied, we can safely use the statistics reported to analyze the fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Loss Function (Ordinary Least Squares)\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/loss.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The coefficient estimates for Ordinary Least Squares rely on the independence of the features. \n",
    "- When features are correlated and the columns of the design matrix \\(X\\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. \n",
    "- This situation of multicollinearity can arise, for example, when data are collected without an experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other loss function for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute or Laplace or L1 loss:\n",
    "$$J_\\theta = \\sum_{i=1}^m|e_i| = \\sum_{i=1}^m|y_t - \\theta_0 - \\theta_1X_1 - \\theta_2X_2|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual sum of squares or L2 loss:\n",
    "$$J_\\theta = \\sum_{i=1}^me_i^2 = \\sum_{i=1}^m(y_t - \\theta_0 - \\theta_1X_1 - \\theta_2X_2)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber loss:\n",
    "\n",
    "It is between L1 and L2 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models with Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization is the process of introducing additional information to minimize overfitting. \n",
    "- Regularization discourages unnecessary complexity.\n",
    "- In regularisation, we try to shrink the regression coefficients by imposing a penalty on their size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Way to Regularize: Add a constraint to the loss function\n",
    "\n",
    "Regularized Loss = Loss Function + Constraint\n",
    "\n",
    "Here constraint = $\\large \\sum_{j=1}^n |\\theta_j|^p$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending upon the value of the parameter $p$ in the constraint, we can have different types of regularisation like L1 (p=1) and L2 (p2). \n",
    "\n",
    "- This is more generally known as Lp regularizer.\n",
    "\n",
    "- For making visualization easy, let us plot them in 2D space. For that we suppose that we just have two parameters.\n",
    "\n",
    "<center><img src=\"images/lp_reg.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso (Least Absolute Shrinkage Selector Operator) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Loss Function + L1 Regularization\n",
    "\n",
    "Here we see that the loss function is modified with an extra term called the penalty term. \n",
    "\n",
    "<center><img src=\"images/l1.png\" width=\"50%\"/></center> \n",
    "\n",
    "$$ L1 = \\large \\lambda \\sum_{j=1}^n|\\theta_j| $$\n",
    "\n",
    "Considering two independent variables, the penalty can be given as = $|\\theta_1| + |\\theta_2|$ .\n",
    "\n",
    "Notice that the intercept $\\theta_0$ has been left out of the penalty term. Penalization of the intercept would make the  procedure depend on the origin chosen for y.\n",
    "\n",
    "- Here $\\lambda$ is a **hyperparameter** and should be _set at the time of model training_. Higher the values of lambda, bigger is the penalty.\n",
    "- L1 shrinks the weights using the absolute values of the weight coefficients (i.e., the weight vector).\n",
    "- Penalize the model by the absolute weight coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/lasso.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The effect of L1 loss\n",
    " - Penalize large coefficients - Large coefficients will increase the size of the total error functions.\n",
    " - L1 regularization induces sparsity. Sparsity means as many values will be zero as possible.\n",
    "\n",
    "#### Why Lasso?\n",
    " - lasso selects only some feature while reduces the coefficients of others to zero. This property is known as **feature selection** and which is absent in case of ridge.\n",
    " - It is generally used when we have more number of features, because it automatically does feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Loss Function + L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/l2.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large L2 = \\lambda \\sum_{j=1}^n\\theta_j^2$$\n",
    "\n",
    "The ridge coefficients minimize a penalized residual sum of squares\n",
    "\n",
    "- Here again $\\lambda$ is a hyperparameter\n",
    "- L2 shrinks the weights by computing the Euclidean norm of the weight coefficients (the weight vector )\n",
    "- it is mostly used to prevent multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/ridge.png\" width=\"50%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both Lasso and Ridge: λ -> Regularization parameter\n",
    "-----\n",
    "\n",
    "λ parameter controls the regularization strength, aka the size of the shaded area.\n",
    "\n",
    "The larger the value of the stronger the regularization of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "Elastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both Ridge and Lasso by tuning the parameters. So it uses both L1 and L2 penality term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/elastic.png\" width=\"75%\"/></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
